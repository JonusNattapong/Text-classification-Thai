{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca43172e",
   "metadata": {},
   "source": [
    "# Thai Sentiment Classification with XLM-RoBERTa\n",
    "\n",
    "This notebook trains a **XLM-RoBERTa** model for Thai text sentiment classification using the Wisesight-Sentiment-Thai dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afb523",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3308ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (4.53.1)\n",
      "Requirement already satisfied: datasets in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: tensorflow in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: requests in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: filelock in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: setuptools in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: namex in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: rich in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: colorama in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\github\\text-classification-thai\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'd:\\Github\\Text-classification-Thai\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30fe1f",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf7fafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5cf0bb",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541f3e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset from Hugging Face ...\n",
      "[INFO] Loading ZombitX64/Wisesight-Sentiment-Thai dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ZombitX64/Wisesight-Sentiment-Thai couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Admin\\.cache\\huggingface\\datasets\\ZombitX64___wisesight-sentiment-thai\\default\\0.0.0\\ce65e1d7831a020c303f6a0e204c43bd351a0f3e (last modified on Mon Jul  7 09:49:35 2025).\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\Admin\\.cache\\huggingface\\datasets\\ZombitX64___wisesight-sentiment-thai\\default\\0.0.0\\ce65e1d7831a020c303f6a0e204c43bd351a0f3e (last modified on Mon Jul  7 09:49:35 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: ['train']\n",
      "Dataset loaded successfully! Size: 628715\n",
      "Column names: ['text', 'sentiment']\n",
      "[INFO] Successfully extracted 628715 texts and 628715 labels\n",
      "Sample text: ‡∏Ñ‡∏≠‡∏Å‡πÄ‡∏ó‡∏•‡∏ï‡∏•‡∏Å‡∏≠‡πà‡∏≤‡∏≤‡∏≤ ‡∏™‡∏á‡∏™‡∏≤‡∏£‡∏û‡∏µ‡πà‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏°‡∏≤‡∏Å55555\n",
      "Sample labels: ['positive', 'question', 'positive', 'positive', 'neutral']\n",
      "[INFO] Dataset loading complete.\n",
      "Total samples: 628715\n",
      "Total labels: 628715\n",
      "\n",
      "Final dataset info:\n",
      "Total samples: 50000\n",
      "Sample texts: ['‡∏Ñ‡∏≠‡∏Å‡πÄ‡∏ó‡∏•‡∏ï‡∏•‡∏Å‡∏≠‡πà‡∏≤‡∏≤‡∏≤ ‡∏™‡∏á‡∏™‡∏≤‡∏£‡∏û‡∏µ‡πà‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏°‡∏≤‡∏Å55555', 'I love you....‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏°‡∏û‡∏π‡πà‡∏ß‡πà‡∏≤‡∏Å‡∏µ‡πà‡∏Ñ‡∏≥‡∏Ñ‡∏£‡∏±‡∏öüòÇ', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏Ñ‡∏£‡∏π‚ù§‡∏ú‡∏°‡∏´‡∏≤‡∏¢‡∏ã‡∏∂‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö']\n",
      "Sample labels: ['positive', 'question', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral']\n",
      "Unique labels: {'neutral', 'question', 'negative', 'positive', 'mixed'}\n",
      "\n",
      "Label distribution:\n",
      "neutral     14541\n",
      "question    12282\n",
      "positive    12149\n",
      "mixed        6608\n",
      "negative     4420\n",
      "Name: count, dtype: int64\n",
      "[INFO] ZombitX64/Wisesight-Sentiment-Thai dataset ready!\n",
      "[INFO] Successfully extracted 628715 texts and 628715 labels\n",
      "Sample text: ‡∏Ñ‡∏≠‡∏Å‡πÄ‡∏ó‡∏•‡∏ï‡∏•‡∏Å‡∏≠‡πà‡∏≤‡∏≤‡∏≤ ‡∏™‡∏á‡∏™‡∏≤‡∏£‡∏û‡∏µ‡πà‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏°‡∏≤‡∏Å55555\n",
      "Sample labels: ['positive', 'question', 'positive', 'positive', 'neutral']\n",
      "[INFO] Dataset loading complete.\n",
      "Total samples: 628715\n",
      "Total labels: 628715\n",
      "\n",
      "Final dataset info:\n",
      "Total samples: 50000\n",
      "Sample texts: ['‡∏Ñ‡∏≠‡∏Å‡πÄ‡∏ó‡∏•‡∏ï‡∏•‡∏Å‡∏≠‡πà‡∏≤‡∏≤‡∏≤ ‡∏™‡∏á‡∏™‡∏≤‡∏£‡∏û‡∏µ‡πà‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏°‡∏≤‡∏Å55555', 'I love you....‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏°‡∏û‡∏π‡πà‡∏ß‡πà‡∏≤‡∏Å‡∏µ‡πà‡∏Ñ‡∏≥‡∏Ñ‡∏£‡∏±‡∏öüòÇ', '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏Ñ‡∏£‡∏π‚ù§‡∏ú‡∏°‡∏´‡∏≤‡∏¢‡∏ã‡∏∂‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö']\n",
      "Sample labels: ['positive', 'question', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'neutral']\n",
      "Unique labels: {'neutral', 'question', 'negative', 'positive', 'mixed'}\n",
      "\n",
      "Label distribution:\n",
      "neutral     14541\n",
      "question    12282\n",
      "positive    12149\n",
      "mixed        6608\n",
      "negative     4420\n",
      "Name: count, dtype: int64\n",
      "[INFO] ZombitX64/Wisesight-Sentiment-Thai dataset ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Loading dataset from Hugging Face ...\")\n",
    "\n",
    "# Load ZombitX64/Wisesight-Sentiment-Thai dataset\n",
    "dataset = None\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "try:\n",
    "    # Method 1: Direct loading with specific configuration\n",
    "    print(\"[INFO] Loading ZombitX64/Wisesight-Sentiment-Thai dataset...\")\n",
    "    \n",
    "    # Try loading without split first to see available splits\n",
    "    dataset_dict = load_dataset('ZombitX64/Wisesight-Sentiment-Thai')\n",
    "    print(f\"Available splits: {list(dataset_dict.keys())}\")\n",
    "    \n",
    "    # Use train split\n",
    "    dataset = dataset_dict['train']\n",
    "    print(f\"Dataset loaded successfully! Size: {len(dataset)}\")\n",
    "    \n",
    "    # Check column names\n",
    "    print(f\"Column names: {dataset.column_names}\")\n",
    "    \n",
    "    # Extract data based on available columns\n",
    "    if 'text' in dataset.column_names:\n",
    "        texts = dataset['text']\n",
    "    elif 'texts' in dataset.column_names:\n",
    "        texts = dataset['texts']\n",
    "    else:\n",
    "        # Find text column\n",
    "        for col in dataset.column_names:\n",
    "            if 'text' in col.lower():\n",
    "                texts = dataset[col]\n",
    "                break\n",
    "    \n",
    "    if 'sentiment' in dataset.column_names:\n",
    "        labels = dataset['sentiment']\n",
    "    elif 'label' in dataset.column_names:\n",
    "        labels = dataset['label']\n",
    "    elif 'category' in dataset.column_names:\n",
    "        labels = dataset['category']\n",
    "    else:\n",
    "        # Find label column\n",
    "        for col in dataset.column_names:\n",
    "            if any(keyword in col.lower() for keyword in ['sentiment', 'label', 'category']):\n",
    "                labels = dataset[col]\n",
    "                break\n",
    "    \n",
    "    print(f\"[INFO] Successfully extracted {len(texts)} texts and {len(labels)} labels\")\n",
    "    print(f\"Sample text: {texts[0] if texts else 'No texts found'}\")\n",
    "    print(f\"Sample labels: {labels[:5] if labels else 'No labels found'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method 1 failed: {e}\")\n",
    "    try:\n",
    "        # Method 2: Try with trust_remote_code and different parameters\n",
    "        print(\"[INFO] Trying alternative loading method...\")\n",
    "        dataset = load_dataset('ZombitX64/Wisesight-Sentiment-Thai', \n",
    "                              split='train',\n",
    "                              trust_remote_code=True,\n",
    "                              download_mode='force_redownload')\n",
    "        \n",
    "        texts = dataset['text'] if 'text' in dataset.column_names else dataset['texts']\n",
    "        labels = dataset['sentiment'] if 'sentiment' in dataset.column_names else dataset['label']\n",
    "        print(\"[INFO] Alternative method successful!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Method 2 failed: {e2}\")\n",
    "        try:\n",
    "            # Method 3: Manual download approach\n",
    "            print(\"[INFO] Trying manual loading approach...\")\n",
    "            \n",
    "            # Use datasets library with specific configuration\n",
    "            from datasets import load_dataset, DownloadConfig\n",
    "            \n",
    "            download_config = DownloadConfig(\n",
    "                resume_download=True,\n",
    "                force_download=False,\n",
    "                use_etag=False\n",
    "            )\n",
    "            \n",
    "            dataset = load_dataset('ZombitX64/Wisesight-Sentiment-Thai',\n",
    "                                 split='train',\n",
    "                                 download_config=download_config)\n",
    "            \n",
    "            # Extract data\n",
    "            texts = dataset['text'] if 'text' in dataset.column_names else dataset['texts']\n",
    "            labels = dataset['sentiment'] if 'sentiment' in dataset.column_names else dataset['label']\n",
    "            print(\"[INFO] Manual loading successful!\")\n",
    "            \n",
    "        except Exception as e3:\n",
    "            print(f\"Method 3 failed: {e3}\")\n",
    "            print(\"[INFO] All methods failed. Please check dataset availability.\")\n",
    "            print(\"Error details:\")\n",
    "            print(f\"- Method 1: {e}\")\n",
    "            print(f\"- Method 2: {e2}\")  \n",
    "            print(f\"- Method 3: {e3}\")\n",
    "            \n",
    "            # Exit if all methods fail\n",
    "            raise Exception(\"Unable to load ZombitX64/Wisesight-Sentiment-Thai dataset\")\n",
    "\n",
    "print(\"[INFO] Dataset loading complete.\")\n",
    "\n",
    "# Verify data quality\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Total labels: {len(labels)}\")\n",
    "\n",
    "if len(texts) != len(labels):\n",
    "    min_len = min(len(texts), len(labels))\n",
    "    texts = texts[:min_len]\n",
    "    labels = labels[:min_len]\n",
    "    print(f\"Adjusted to {min_len} samples for consistency\")\n",
    "\n",
    "# Use subset for training (adjust based on your resources)\n",
    "SAMPLE_SIZE = min(50000, len(texts))  # Use up to 50k samples\n",
    "texts = texts[:SAMPLE_SIZE]\n",
    "labels = labels[:SAMPLE_SIZE]\n",
    "\n",
    "print(f\"\\nFinal dataset info:\")\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "print(f\"Sample texts: {texts[:3]}\")\n",
    "print(f\"Sample labels: {labels[:10]}\")\n",
    "print(f\"Unique labels: {set(labels)}\")\n",
    "\n",
    "# Show label distribution\n",
    "import pandas as pd\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "print(f\"\\nLabel distribution:\\n{label_counts}\")\n",
    "\n",
    "print(\"[INFO] ZombitX64/Wisesight-Sentiment-Thai dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d31bce",
   "metadata": {},
   "source": [
    "## 4. Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1bf0d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Encoding labels ...\n",
      "Total samples: 50000\n",
      "Label classes: ['mixed' 'negative' 'neutral' 'positive' 'question']\n",
      "Number of classes: 5\n",
      "\n",
      "Label distribution:\n",
      "neutral     14541\n",
      "question    12282\n",
      "positive    12149\n",
      "mixed        6608\n",
      "negative     4420\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Encoding labels ...\")\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "print(f\"Total samples: {len(labels)}\")\n",
    "print(f\"Label classes: {le.classes_}\")\n",
    "print(f\"Number of classes: {num_labels}\")\n",
    "\n",
    "# Check label distribution\n",
    "import pandas as pd\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "print(f\"\\nLabel distribution:\\n{label_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94d8d4",
   "metadata": {},
   "source": [
    "## 5. Load Tokenizer and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bc59d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading tokenizer: xlm-roberta-base\n",
      "[INFO] Tokenizing texts ...\n",
      "[INFO] Tokenizing texts ...\n",
      "[INFO] Tokenization complete.\n",
      "Input IDs shape: (50000, 128)\n",
      "Attention mask shape: (50000, 128)\n",
      "Sample tokens (first 10): [     0  27704  40143 189865 216248   5407  23560   9250   9250  14990]\n",
      "All sequences length 128: True\n",
      "[INFO] Tokenization complete.\n",
      "Input IDs shape: (50000, 128)\n",
      "Attention mask shape: (50000, 128)\n",
      "Sample tokens (first 10): [     0  27704  40143 189865 216248   5407  23560   9250   9250  14990]\n",
      "All sequences length 128: True\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"xlm-roberta-base\"  # Multilingual XLM-RoBERTa (supports Thai)\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "print(f\"[INFO] Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"[INFO] Tokenizing texts ...\")\n",
    "# Fix: Use return_tensors='np' and ensure consistent padding\n",
    "encodings = tokenizer(\n",
    "    texts, \n",
    "    truncation=True, \n",
    "    padding='max_length',  # Changed from padding=True to padding='max_length'\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='np'  # Return numpy arrays for consistent shapes\n",
    ")\n",
    "print(\"[INFO] Tokenization complete.\")\n",
    "\n",
    "print(f\"Input IDs shape: {encodings['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {encodings['attention_mask'].shape}\")\n",
    "print(f\"Sample tokens (first 10): {encodings['input_ids'][0][:10]}\")\n",
    "\n",
    "# Verify all sequences have the same length\n",
    "print(f\"All sequences length {MAX_LENGTH}: {all(len(seq) == MAX_LENGTH for seq in encodings['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07725214",
   "metadata": {},
   "source": [
    "## 6. Create TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d886f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preparing tf.data.Dataset ...\n",
      "Input IDs tensor shape: (50000, 128)\n",
      "Attention mask tensor shape: (50000, 128)\n",
      "Labels tensor shape: (50000,)\n",
      "[INFO] Dataset ready for training. Train size: 40000, Val size: 10000\n",
      "Batch input_ids shape: (16, 128)\n",
      "Batch attention_mask shape: (16, 128)\n",
      "Batch labels shape: (16,)\n",
      "[INFO] Dataset ready for training. Train size: 40000, Val size: 10000\n",
      "Batch input_ids shape: (16, 128)\n",
      "Batch attention_mask shape: (16, 128)\n",
      "Batch labels shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "# Convert to tf.data.Dataset using numpy arrays (more reliable)\n",
    "print(\"[INFO] Preparing tf.data.Dataset ...\")\n",
    "\n",
    "# Convert to tensorflow tensors directly\n",
    "input_ids = tf.constant(encodings['input_ids'], dtype=tf.int32)\n",
    "attention_mask = tf.constant(encodings['attention_mask'], dtype=tf.int32)\n",
    "labels_tensor = tf.constant(labels_encoded, dtype=tf.int64)\n",
    "\n",
    "print(f\"Input IDs tensor shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask tensor shape: {attention_mask.shape}\")\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")\n",
    "\n",
    "# Create dataset from tensors\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "    'labels': labels_tensor\n",
    "})\n",
    "\n",
    "# Split into train/validation\n",
    "BATCH_SIZE = 16\n",
    "dataset_size = len(texts)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "\n",
    "# Shuffle and split\n",
    "dataset = dataset.shuffle(buffer_size=10000, seed=42)\n",
    "train_ds = dataset.take(train_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = dataset.skip(train_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"[INFO] Dataset ready for training. Train size: {train_size}, Val size: {dataset_size - train_size}\")\n",
    "\n",
    "# Test the dataset structure\n",
    "for batch in train_ds.take(1):\n",
    "    print(f\"Batch input_ids shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"Batch attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"Batch labels shape: {batch['labels'].shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ceb0c5",
   "metadata": {},
   "source": [
    "## 7. Load and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1c7670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading pre-trained model ...\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded and compiled.\n",
      "Model summary:\n",
      "Model: \"tfxlm_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFXLMRobertaMainL  multiple                  277453056 \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " classifier (TFXLMRobertaCl  multiple                  594437    \n",
      " assificationHead)                                               \n",
      "                                                                 \n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFXLMRobertaMainL  multiple                  277453056 \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " classifier (TFXLMRobertaCl  multiple                  594437    \n",
      " assificationHead)                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 278047493 (1.04 GB)\n",
      "Trainable params: 278047493 (1.04 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "=================================================================\n",
      "Total params: 278047493 (1.04 GB)\n",
      "Trainable params: 278047493 (1.04 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Loading pre-trained model ...\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"[INFO] Model loaded and compiled.\")\n",
    "print(f\"Model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4156c",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b64ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training ...\n",
      "Epoch 1/3\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Github\\Text-classification-Thai\\.venv\\lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "  11/2500 [..............................] - ETA: 8:43:35 - loss: 1.6224 - accuracy: 0.1648"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Training ...\")\n",
    "\n",
    "# Add callbacks for better monitoring\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.5)\n",
    "]\n",
    "\n",
    "# Prepare the datasets for training (extract labels from the dataset)\n",
    "def extract_labels(batch):\n",
    "    return {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask']}, batch['labels']\n",
    "\n",
    "train_ds_formatted = train_ds.map(extract_labels)\n",
    "val_ds_formatted = val_ds.map(extract_labels)\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 3  # Increase if you have more time/resources\n",
    "history = model.fit(\n",
    "    train_ds_formatted,\n",
    "    validation_data=val_ds_formatted,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"[INFO] Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9f893",
   "metadata": {},
   "source": [
    "## 9. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history.history['loss'], label='Train Loss')\n",
    "ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d074227",
   "metadata": {},
   "source": [
    "## 10. Test Model with Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcc109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with sample text\n",
    "def predict_sentiment(text, model, tokenizer, label_encoder, max_len=128):\n",
    "    inputs = tokenizer(text, truncation=True, padding='max_length', max_length=max_len, return_tensors='tf')\n",
    "    logits = model(inputs)[0]\n",
    "    probs = tf.nn.softmax(logits, axis=1).numpy()[0]\n",
    "    pred_idx = np.argmax(probs)\n",
    "    pred_label = label_encoder.classes_[pred_idx]\n",
    "    confidence = probs[pred_idx]\n",
    "    top_indices = np.argsort(probs)[::-1][:3]\n",
    "    top_labels = [label_encoder.classes_[i] for i in top_indices]\n",
    "    top_scores = [probs[i] for i in top_indices]\n",
    "    return pred_label, confidence, top_labels, top_scores\n",
    "\n",
    "test_texts = [\n",
    "    \"‡∏£‡πâ‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏£‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å ‡∏ä‡∏≠‡∏ö‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢\",  # Should be positive\n",
    "    \"‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡πà ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏≠‡∏£‡πà‡∏≠‡∏¢\",  # Should be negative\n",
    "    \"‡∏õ‡∏Å‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏î‡∏µ‡πÑ‡∏°‡πà‡πÅ‡∏¢‡πà\",  # Should be neutral\n",
    "    \"‡∏î‡∏µ‡πÉ‡∏à‡∏°‡∏≤‡∏Å ‡∏õ‡∏£‡∏∞‡∏ó‡∏±‡∏ö‡πÉ‡∏à\",  # Should be positive\n",
    "    \"‡πÄ‡∏®‡∏£‡πâ‡∏≤ ‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á\",  # Should be negative\n",
    "    \"‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞‡∏ï‡∏≠‡∏ö‡∏≠‡∏∞‡πÑ‡∏£\",  # Should be question/mixed\n",
    "    \"‡∏´‡∏ô‡∏±‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á ‡∏°‡∏µ‡∏Ñ‡∏ô‡∏î‡∏π‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏±‡πâ‡∏¢\"  # Should be question\n",
    "]\n",
    "\n",
    "print(\"\\n=== Testing Transformer Model ===\")\n",
    "for i, text in enumerate(test_texts):\n",
    "    predicted_label, confidence, top_labels, top_scores = predict_sentiment(text, model, tokenizer, le)\n",
    "    print(f\"\\nTest {i+1}: {text}\")\n",
    "    print(f\"Predicted sentiment: {predicted_label} (confidence: {confidence:.4f})\")\n",
    "    print(f\"Top 3 predictions:\")\n",
    "    for label, score in zip(top_labels, top_scores):\n",
    "        print(f\"  {label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b76af8",
   "metadata": {},
   "source": [
    "## 11. Save Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] Saving model and tokenizer ...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained('/content/thai_text_transformer_model')\n",
    "tokenizer.save_pretrained('/content/thai_text_transformer_model')\n",
    "\n",
    "# Save label encoder\n",
    "with open('/content/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"[INFO] Model, tokenizer, and label encoder saved successfully!\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- /content/thai_text_transformer_model/ (model and tokenizer)\")\n",
    "print(\"- /content/label_encoder.pkl (label encoder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7dceb",
   "metadata": {},
   "source": [
    "## 12. Download Files (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cefc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip file for easy download\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Creating zip file for download...\")\n",
    "shutil.make_archive('/content/thai_sentiment_model', 'zip', '/content/', 'thai_text_transformer_model')\n",
    "\n",
    "# Download files\n",
    "print(\"Downloading model files...\")\n",
    "files.download('/content/thai_sentiment_model.zip')\n",
    "files.download('/content/label_encoder.pkl')\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25782989",
   "metadata": {},
   "source": [
    "## 13. Load and Test Saved Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3beb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading the saved model\n",
    "print(\"Testing saved model...\")\n",
    "\n",
    "# Load saved model\n",
    "loaded_model = TFAutoModelForSequenceClassification.from_pretrained('/content/thai_text_transformer_model')\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('/content/thai_text_transformer_model')\n",
    "with open('/content/label_encoder.pkl', 'rb') as f:\n",
    "    loaded_le = pickle.load(f)\n",
    "\n",
    "# Test with a sample\n",
    "test_text = \"‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏Å‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏°‡∏≤‡∏Å\"\n",
    "pred_label, confidence, top_labels, top_scores = predict_sentiment(test_text, loaded_model, loaded_tokenizer, loaded_le)\n",
    "\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Predicted: {pred_label} (confidence: {confidence:.4f})\")\n",
    "print(\"Model loaded and working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691ec03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "1. ‚úÖ Loaded the Wisesight-Sentiment-Thai dataset\n",
    "2. ‚úÖ Fine-tuned XLM-RoBERTa for Thai sentiment classification\n",
    "3. ‚úÖ Trained the model with validation monitoring\n",
    "4. ‚úÖ Tested the model with sample Thai texts\n",
    "5. ‚úÖ Saved the model, tokenizer, and label encoder\n",
    "\n",
    "**Model Performance:**\n",
    "- Labels: mixed, negative, neutral, positive, question\n",
    "- Architecture: XLM-RoBERTa (multilingual BERT)\n",
    "- Max Length: 128 tokens\n",
    "- Can be uploaded to Hugging Face Hub for public use\n",
    "\n",
    "**Next Steps:**\n",
    "- Upload to Hugging Face Hub\n",
    "- Deploy as API or web service\n",
    "- Fine-tune with domain-specific data\n",
    "- Compare with other models (Thai-specific BERT, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
